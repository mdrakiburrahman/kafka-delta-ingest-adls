{"cells":[{"cell_type":"code","source":["# - - - - - - - - - - -\n# Inject from DLT Job\n# - - - - - - - - - - -\ntry:\n  spark.conf.get(\"kdiPipeline.KAFKA_BROKER_ADDRESS\")\n  spark.conf.get(\"kdiPipeline.KAFKA_CONSUMER_NAME_SELF\")\n  spark.conf.get(\"kdiPipeline.KAFKA_TOPIC_LIST\")\n  spark.conf.get(\"kdiPipeline.PRIMARY_KEY_LIST\")\nexcept:\n  spark.conf.set(\"kdiPipeline.KAFKA_BROKER_ADDRESS\", \"kafka:9092\")\n  spark.conf.set(\"kdiPipeline.KAFKA_CONSUMER_NAME_SELF\", \"kdi-java-1\")\n  spark.conf.set(\"kdiPipeline.KAFKA_TOPIC_LIST\", \"server1.dbo.customers|server1.dbo.products_on_hand\")\n  spark.conf.set(\"kdiPipeline.PRIMARY_KEY_LIST\", \"id|product_id\")\n\n# - - - - -\n# Injected\n# - - - - -\n# Single\nKAFKA_BROKER_ADDRESS = spark.conf.get(\"kdiPipeline.KAFKA_BROKER_ADDRESS\")\nKAFKA_CONSUMER_NAME_SELF = spark.conf.get(\"kdiPipeline.KAFKA_CONSUMER_NAME_SELF\")\n\n# List\nKAFKA_TOPIC_LIST = spark.conf.get(\"kdiPipeline.KAFKA_TOPIC_LIST\").split(\"|\")\nPRIMARY_KEY_LIST = spark.conf.get(\"kdiPipeline.PRIMARY_KEY_LIST\").split(\"|\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Parameter setup","showTitle":true,"inputWidgets":{},"nuid":"f77a387d-1958-4af6-a30b-3f1c59824b53"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import dlt\nimport json\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *\nfrom pyspark.context import SparkContext\n\ndef generate_silver(\n  kafka_topic, kafka_broker_address, kafka_consumer_name_self,\n  cdc_staging_table, bronze_table, upsert_staging_view, silver_table, primary_keys\n):\n  # - - - - - - - - - - - - \n  # Table scoped variables\n  # - - - - - - - - - - - - \n  cdc_raw_delta_path = f\"/mnt/cdckafka/{kafka_broker_address}/{kafka_topic}/kdi={kafka_consumer_name_self}\"\n                    \n  # - - - - - - - - - - - - - - - - - - - - - - - - - -\n  # Read JSON Payload -> Dictionary -> List -> Schema\n  # - - - - - - - - - - - - - - - - - - - - - - - - - -\n  with open(f\"/dbfs/mnt/cdckafka/kafka_payloads/{kafka_topic}.json\", 'r') as f:\n      jsonString = f.read()\n  Dict = json.loads(jsonString)\n  jsonData = json.dumps(Dict)\n  jsonDataList = []\n  jsonDataList.append(jsonData)\n\n  # Read List into Dataframe\n  sc = spark.sparkContext # Pull out Sparkcontext\n  jsonRDD = sc.parallelize(jsonDataList)\n  df = spark.read.json(jsonRDD)\n  schema = df.schema\n\n  # - - - - - - - - - - - - \n  # Ingest raw CDC data\n  # - - - - - - - - - - - - \n  @dlt.create_table(\n    name=cdc_staging_table,\n    comment=\"The raw Debezium logs from Kafka fed via KDI Container.\"\n  )\n  def cdc_raw():          \n    return (\n      spark.readStream.format(\"delta\").load(cdc_raw_delta_path)\n    )\n\n  # - - - - - - - - - - - - - - - - - - - - \n  # Exploded JSON with inferred Schema\n  # - - - - - - - - - - - - - - - - - - - - \n  @dlt.table(\n    name=bronze_table,\n    comment=\"Exploded JSON with Schema inference.\"\n  )\n  def create_bronze_table():\n    return (\n      dlt.read_stream(cdc_staging_table)\n        .select(\"*\", from_json(col(\"value\"), schema).alias(\"data\")) \n        .select(\"*\", \"data.*\")\n        .withColumn(\"SourceModifiedDate\", from_unixtime((col(\"payload.ts_ms\") / 1000)))\n        .select(\"SourceModifiedDate\", \"payload.*\")\n        .filter(\"op is not null\")\n    )\n\n  # - - - - - - - - -\n  # Staging View\n  # - - - - - - - - -\n  @dlt.view(\n    name=upsert_staging_view,\n    comment=\"Forming a staging view ready for UPSERT.\"\n  )\n  def create_upsert_stage_view():\n      return (\n        dlt.read_stream(bronze_table)\n          .withColumn(\"result\", when(col(\"op\") == \"d\", col(\"before\"))\n                               .otherwise(col(\"after\")))\n          .select(\"SourceModifiedDate\", \"op\", \"result.*\")\n      )\n\n  # - - - - - - - - -\n  # SCD 1 to Silver\n  # - - - - - - - - -\n  dlt.create_target_table(silver_table)\n\n  dlt.apply_changes(\n    target = silver_table,\n    source = upsert_staging_view,\n    keys = primary_keys,\n    sequence_by = col(\"SourceModifiedDate\"),\n    apply_as_deletes = expr(\"op = 'd'\"),\n    except_column_list = [\"op\", \"SourceModifiedDate\"]\n  )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"DLT Pipeline Definition","showTitle":true,"inputWidgets":{},"nuid":"d1cb6843-eb28-48ad-b383-77db5f52ab60"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Loop over for all tables\nfor i in range(len(KAFKA_TOPIC_LIST)):\n  # - - - - - - - - - - - - \n  # Table scoped variables\n  # - - - - - - - - - - - - \n  TABLE_NAME = KAFKA_TOPIC_LIST[i].replace(\".\", \"_\")\n  CDC_STAGING_TABLE = f\"CDC_{TABLE_NAME}\"\n  BRONZE_TABLE = f\"BRONZE_{TABLE_NAME}\"\n  UPSERT_STAGING_VIEW = f\"UPSERT_{TABLE_NAME}\"\n  SILVER_TABLE = f\"SILVER_{TABLE_NAME}\"\n  \n  # We need to pass in a list as a Primary key\n  LIST = [None] * 1 # <- We assume to start a table has one unique PK\n  LIST[0] = PRIMARY_KEY_LIST[i]\n  \n  # - - - - - - - - - - - - - -\n  # Call DLT Pipeline per table\n  # - - - - - - - - - - - - - -\n  generate_silver(\n    kafka_topic=KAFKA_TOPIC_LIST[i], \n    kafka_broker_address=KAFKA_BROKER_ADDRESS, \n    kafka_consumer_name_self=KAFKA_CONSUMER_NAME_SELF,\n    cdc_staging_table=CDC_STAGING_TABLE, \n    bronze_table=BRONZE_TABLE, \n    upsert_staging_view=UPSERT_STAGING_VIEW, \n    silver_table=SILVER_TABLE, \n    primary_keys=LIST\n  )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"DLT Pipeline Call","showTitle":true,"inputWidgets":{},"nuid":"68de110f-d4a7-4b0e-a9bb-ac1b74665a3c"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Debezium-DLT-dynamic","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2007605345394028}},"nbformat":4,"nbformat_minor":0}
